{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a703c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 policy: [0.3329716911964922, 0.33477133113423474, 0.3322569776692732]\n",
      "player 2 policy: [0.33262953143817625, 0.3348624514209909, 0.33250801714083283]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "\n",
    "\n",
    "class RPSTrainer:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.NUM_ACTIONS = 3\n",
    "        self.possible_actions = np.arange(self.NUM_ACTIONS)\n",
    "        # Order left to right, and up to down is Rock, Paper, Scissors\n",
    "        self.actionUtility = np.array([\n",
    "                    [0, -1, 1],\n",
    "                    [1, 0, -1],\n",
    "                    [-1, 1, 0]\n",
    "                ])\n",
    "        self.regret_sum = np.zeros(self.NUM_ACTIONS)\n",
    "        self.strategy_sum = np.zeros(self.NUM_ACTIONS)\n",
    "\n",
    "        self.opponent_regret_sum = np.zeros(self.NUM_ACTIONS)\n",
    "        self.opponent_strategy_sum = np.zeros(self.NUM_ACTIONS)\n",
    "\n",
    "    def get_strategy(self, regret_sum):\n",
    "        new_sum = np.clip(regret_sum, a_min=0, a_max=None)\n",
    "        normalizing_sum = np.sum(new_sum)\n",
    "        if normalizing_sum > 0:\n",
    "            new_sum /= normalizing_sum\n",
    "        else:\n",
    "            new_sum = np.repeat(1/self.NUM_ACTIONS, self.NUM_ACTIONS)\n",
    "        return new_sum\n",
    "\n",
    "    def get_average_strategy(self, strategy_sum):\n",
    "        average_strategy = [0, 0, 0]\n",
    "        normalizing_sum = sum(strategy_sum)\n",
    "        for a in range(self.NUM_ACTIONS):\n",
    "            if normalizing_sum > 0:\n",
    "                average_strategy[a] = strategy_sum[a] / normalizing_sum\n",
    "            else:\n",
    "                average_strategy[a] = 1.0 / self.NUM_ACTIONS\n",
    "        return average_strategy\n",
    "\n",
    "    def get_action(self, strategy):\n",
    "        return choice(self.possible_actions, p=strategy)\n",
    "\n",
    "    def get_reward(self, my_action, opponent_action):\n",
    "        return self.actionUtility[my_action, opponent_action]\n",
    "\n",
    "    def train(self, iterations):\n",
    "\n",
    "        for i in range(iterations):\n",
    "            strategy = self.get_strategy(self.regret_sum)\n",
    "            opp_strategy = self.get_strategy(self.opponent_regret_sum)\n",
    "            self.strategy_sum += strategy\n",
    "            self.opponent_strategy_sum += opp_strategy\n",
    "\n",
    "            opponent_action = self.get_action(opp_strategy)\n",
    "            my_action = self.get_action(strategy)\n",
    "\n",
    "            my_reward = self.get_reward(my_action, opponent_action)\n",
    "            opp_reward = self.get_reward(opponent_action, my_action)\n",
    "\n",
    "            for a in range(self.NUM_ACTIONS):\n",
    "                my_regret = self.get_reward(a, opponent_action) - my_reward\n",
    "                opp_regret = self.get_reward(a, my_action) - opp_reward\n",
    "                self.regret_sum[a] += my_regret\n",
    "                self.opponent_regret_sum[a] += opp_regret\n",
    "\n",
    "\n",
    "def main():\n",
    "    trainer = RPSTrainer()\n",
    "    trainer.train(100000)\n",
    "    target_policy = trainer.get_average_strategy(trainer.strategy_sum)\n",
    "    opp_target_policy = trainer.get_average_strategy(trainer.opponent_strategy_sum)\n",
    "    print('player 1 policy: %s' % target_policy)\n",
    "    print('player 2 policy: %s' % opp_target_policy)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db061f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 policy: [0.5, 0.5]\n",
      "player 2 policy: [0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "\n",
    "\n",
    "class FCTrainer:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.NUM_ACTIONS = 2\n",
    "        self.possible_actions = np.arange(self.NUM_ACTIONS)\n",
    "        # Order left to right, and up to down is Heads and Tails\n",
    "        self.actionUtility = np.array([\n",
    "                    [1, -1,],\n",
    "                    [1, -1,]\n",
    "                ])\n",
    "        self.regret_sum = np.zeros(self.NUM_ACTIONS)\n",
    "        self.strategy_sum = np.zeros(self.NUM_ACTIONS)\n",
    "\n",
    "        self.opponent_regret_sum = np.zeros(self.NUM_ACTIONS)\n",
    "        self.opponent_strategy_sum = np.zeros(self.NUM_ACTIONS)\n",
    "\n",
    "    def get_strategy(self, regret_sum):\n",
    "        new_sum = np.clip(regret_sum, a_min=0, a_max=None)\n",
    "        normalizing_sum = np.sum(new_sum)\n",
    "        if normalizing_sum > 0:\n",
    "            new_sum /= normalizing_sum\n",
    "        else:\n",
    "            new_sum = np.repeat(1/self.NUM_ACTIONS, self.NUM_ACTIONS)\n",
    "        return new_sum\n",
    "\n",
    "    def get_average_strategy(self, strategy_sum):\n",
    "        average_strategy = [0, 0]\n",
    "        normalizing_sum = sum(strategy_sum)\n",
    "        for a in range(self.NUM_ACTIONS):\n",
    "            if normalizing_sum > 0:\n",
    "                average_strategy[a] = strategy_sum[a] / normalizing_sum\n",
    "            else:\n",
    "                average_strategy[a] = 1.0 / self.NUM_ACTIONS\n",
    "        return average_strategy\n",
    "    \n",
    "    def get_action(self, strategy):\n",
    "        return choice(self.possible_actions, p=strategy)\n",
    "\n",
    "    def get_reward(self, my_action, opponent_action):\n",
    "        return self.actionUtility[my_action, opponent_action]\n",
    "\n",
    "    def train(self, iterations):\n",
    "\n",
    "        for i in range(iterations):\n",
    "            strategy = self.get_strategy(self.regret_sum)\n",
    "            opp_strategy = self.get_strategy(self.opponent_regret_sum)\n",
    "            self.strategy_sum += strategy\n",
    "            self.opponent_strategy_sum += opp_strategy\n",
    "\n",
    "            opponent_action = self.get_action(opp_strategy)\n",
    "            my_action = self.get_action(strategy)\n",
    "\n",
    "            my_reward = self.get_reward(my_action, opponent_action)\n",
    "            opp_reward = self.get_reward(opponent_action, my_action)\n",
    "\n",
    "            for a in range(self.NUM_ACTIONS):\n",
    "                my_regret = self.get_reward(a, opponent_action) - my_reward\n",
    "                opp_regret = self.get_reward(a, my_action) - opp_reward\n",
    "                self.regret_sum[a] += my_regret\n",
    "                self.opponent_regret_sum[a] += opp_regret\n",
    "\n",
    "\n",
    "def main():\n",
    "    trainer = FCTrainer()\n",
    "    trainer.train(1000)\n",
    "    target_policy = trainer.get_average_strategy(trainer.strategy_sum)\n",
    "    opp_target_policy = trainer.get_average_strategy(trainer.opponent_strategy_sum)\n",
    "    print('player 1 policy: %s' % target_policy)\n",
    "    print('player 2 policy: %s' % opp_target_policy)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c983284c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 policy: [0.19996059189375384, 0.2010574434610152, 0.19948202741812673, 0.2002652507377983, 0.19923468648930587]\n",
      "player 2 policy: [0.20383175103564422, 0.2047558728576919, 0.19951046831117386, 0.19459898473769208, 0.19730292305779792]\n"
     ]
    }
   ],
   "source": [
    "class RPSLSTrainer:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.NUM_ACTIONS = 5\n",
    "        self.possible_actions = np.arange(self.NUM_ACTIONS)\n",
    "        # Order left to right, and up to down is Rock, Paper, Scissors, Lizard, Spock\n",
    "        self.actionUtility = np.array([\n",
    "                    [0, -1, 1, 1, -1],\n",
    "                    [1, 0, -1, -1, 1],\n",
    "                    [-1, 1, 0, 1, -1],\n",
    "                    [-1, 1, -1, 0, 1],\n",
    "                    [1, -1, 1, -1, 0]\n",
    "                ])\n",
    "        self.regret_sum = np.zeros(self.NUM_ACTIONS)\n",
    "        self.strategy_sum = np.zeros(self.NUM_ACTIONS)\n",
    "\n",
    "        self.opponent_regret_sum = np.zeros(self.NUM_ACTIONS)\n",
    "        self.opponent_strategy_sum = np.zeros(self.NUM_ACTIONS)\n",
    "\n",
    "    def get_strategy(self, regret_sum):\n",
    "        new_sum = np.clip(regret_sum, a_min=0, a_max=None)\n",
    "        normalizing_sum = np.sum(new_sum)\n",
    "        if normalizing_sum > 0:\n",
    "            new_sum /= normalizing_sum\n",
    "        else:\n",
    "            new_sum = np.repeat(1/self.NUM_ACTIONS, self.NUM_ACTIONS)\n",
    "        return new_sum\n",
    "\n",
    "    def get_average_strategy(self, strategy_sum):\n",
    "        average_strategy = [0, 0, 0, 0, 0]\n",
    "        normalizing_sum = sum(strategy_sum)\n",
    "        for a in range(self.NUM_ACTIONS):\n",
    "            if normalizing_sum > 0:\n",
    "                average_strategy[a] = strategy_sum[a] / normalizing_sum\n",
    "            else:\n",
    "                average_strategy[a] = 1.0 / self.NUM_ACTIONS\n",
    "        return average_strategy\n",
    "    \n",
    "    def get_action(self, strategy):\n",
    "        return choice(self.possible_actions, p=strategy)\n",
    "\n",
    "    def get_reward(self, my_action, opponent_action):\n",
    "        return self.actionUtility[my_action, opponent_action]\n",
    "\n",
    "    def train(self, iterations):\n",
    "\n",
    "        for i in range(iterations):\n",
    "            strategy = self.get_strategy(self.regret_sum)\n",
    "            opp_strategy = self.get_strategy(self.opponent_regret_sum)\n",
    "            self.strategy_sum += strategy\n",
    "            self.opponent_strategy_sum += opp_strategy\n",
    "\n",
    "            opponent_action = self.get_action(opp_strategy)\n",
    "            my_action = self.get_action(strategy)\n",
    "\n",
    "            my_reward = self.get_reward(my_action, opponent_action)\n",
    "            opp_reward = self.get_reward(opponent_action, my_action)\n",
    "\n",
    "            for a in range(self.NUM_ACTIONS):\n",
    "                my_regret = self.get_reward(a, opponent_action) - my_reward\n",
    "                opp_regret = self.get_reward(a, my_action) - opp_reward\n",
    "                self.regret_sum[a] += my_regret\n",
    "                self.opponent_regret_sum[a] += opp_regret\n",
    "\n",
    "\n",
    "def main():\n",
    "    trainer = RPSLSTrainer()\n",
    "    trainer.train(100000)\n",
    "    target_policy = trainer.get_average_strategy(trainer.strategy_sum)\n",
    "    opp_target_policy = trainer.get_average_strategy(trainer.opponent_strategy_sum)\n",
    "    print('player 1 policy: %s' % target_policy)\n",
    "    print('player 2 policy: %s' % opp_target_policy)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d035b16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 policy: [0.001, 0.001, 0.001, 0.001, 0.01296925250032232, 0.006986029274360108, 0.0031003332810680927, 0.031906818528839725, 0.4526331945623697, 0.48840437185304003]\n",
      "player 2 policy: [0.001, 0.001, 0.001, 0.001, 0.0063488264862158516, 0.012568059386965185, 0.03316730038226133, 0.2506172200525107, 0.34515665202913426, 0.3481419416629127]\n"
     ]
    }
   ],
   "source": [
    "class RPSLSTrainer:\n",
    "    def __init__(self):\n",
    "        rf= 0.00000154\n",
    "        sfl= 0.000039\n",
    "        kf= 0.00024\n",
    "        fh= 0.001441\n",
    "        fl= 0.001965\n",
    "        st= 0.003925\n",
    "        k3= 0.021128\n",
    "        p2= 0.047539\n",
    "        k2= 0.422569\n",
    "        hc= 0.501177\n",
    "        self.NUM_ACTIONS = 10\n",
    "        self.possible_actions = np.arange(self.NUM_ACTIONS)\n",
    "        # Order left to right, and up to down is \n",
    "        self.actionUtility = np.array([\n",
    "                    [0   , -1*k2,  -1*p2, -1*k3, -1*st, -1*fl, -1*fh, -1, -1, -1],\n",
    "                    [1*k2,    0,   -1*k2*p2, -1*k2*k3, -1*k2*st,-1*k2*fl, -1*k2*fh, -1*k2, -1*k2, -1*k2],\n",
    "                    [1*p2, 1*p2*k2,    0, -1*p2*k3, -1*p2*st, -1*p2*fl, -1*p2*fh, -1*p2, -1*p2, -1*p2],\n",
    "                    [1*k3, 1*k3*k2, 1*k3*p2, 0, -1*k3*st, -1*k3*fl, -1*k3*fh, -1*k3, -1*k3, -1*k3],\n",
    "                    [1*st, 1*st*k2, 1*st*p2, 1*st*k3, 0, -1*st*fl, -1*st*fh, -1*st, -1*st, -1*st],\n",
    "                    [1*fl, 1*fl*k2, 1*fl*p2, 1*fl*k3, 1*fl*st, 0, -1*fl*fh, -1*fl, -1*fl, -1*fl],\n",
    "                    [1*fh, 1*fh*k2, 1*fh*p2, 1*fh*k3, 1*fh*st, 1*fh*fl, 0, -1*fh, -1*fh, -1*fh],\n",
    "                    [1*kf, 1*kf*k2, 1*kf*p2, 1*kf*k3, 1*kf*st, 1*kf*fl, 1*kf*fh, 0, -1*kf, -1*kf],\n",
    "                    [1*sfl, 1*sfl*k2, 1*sfl*p2, 1*sfl*k3, 1*sfl*st, 1*sfl*fl, 1*sfl*fh, 1*sfl, 0, -1*sfl],\n",
    "                    [1*rf, 1*rf*k2, 1*rf*p2, 1*rf*k3, 1*rf*st, 1*rf*fl, 1*rf, 1*rf*fh, 1*rf, 0]\n",
    "                ])\n",
    "        self.regret_sum = np.zeros(self.NUM_ACTIONS)\n",
    "        self.strategy_sum = np.zeros(self.NUM_ACTIONS)\n",
    "\n",
    "        self.opponent_regret_sum = np.zeros(self.NUM_ACTIONS)\n",
    "        self.opponent_strategy_sum = np.zeros(self.NUM_ACTIONS)\n",
    "\n",
    "    def get_strategy(self, regret_sum):\n",
    "        new_sum = np.clip(regret_sum, a_min=0, a_max=None)\n",
    "        normalizing_sum = np.sum(new_sum)\n",
    "        if normalizing_sum > 0:\n",
    "            new_sum /= normalizing_sum\n",
    "        else:\n",
    "            new_sum = np.repeat(1/self.NUM_ACTIONS, self.NUM_ACTIONS)\n",
    "        return new_sum\n",
    "\n",
    "    def get_average_strategy(self, strategy_sum):\n",
    "        average_strategy = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        normalizing_sum = sum(strategy_sum)\n",
    "        for a in range(self.NUM_ACTIONS):\n",
    "            if normalizing_sum > 0:\n",
    "                average_strategy[a] = strategy_sum[a] / normalizing_sum\n",
    "            else:\n",
    "                average_strategy[a] = 1.0 / self.NUM_ACTIONS\n",
    "        return average_strategy\n",
    "    \n",
    "    def get_action(self, strategy):\n",
    "        return choice(self.possible_actions, p=strategy)\n",
    "\n",
    "    def get_reward(self, my_action, opponent_action):\n",
    "        return self.actionUtility[my_action, opponent_action]\n",
    "\n",
    "    def train(self, iterations):\n",
    "\n",
    "        for i in range(iterations):\n",
    "            strategy = self.get_strategy(self.regret_sum)\n",
    "            opp_strategy = self.get_strategy(self.opponent_regret_sum)\n",
    "            self.strategy_sum += strategy\n",
    "            self.opponent_strategy_sum += opp_strategy\n",
    "\n",
    "            opponent_action = self.get_action(opp_strategy)\n",
    "            my_action = self.get_action(strategy)\n",
    "\n",
    "            my_reward = self.get_reward(my_action, opponent_action)\n",
    "            opp_reward = self.get_reward(opponent_action, my_action)\n",
    "\n",
    "            for a in range(self.NUM_ACTIONS):\n",
    "                my_regret = self.get_reward(a, opponent_action) - my_reward\n",
    "                opp_regret = self.get_reward(a, my_action) - opp_reward\n",
    "                self.regret_sum[a] += my_regret\n",
    "                self.opponent_regret_sum[a] += opp_regret\n",
    "\n",
    "\n",
    "def main():\n",
    "    trainer = RPSLSTrainer()\n",
    "    trainer.train(100)\n",
    "    target_policy = trainer.get_average_strategy(trainer.strategy_sum)\n",
    "    opp_target_policy = trainer.get_average_strategy(trainer.opponent_strategy_sum)\n",
    "    print('player 1 policy: %s' % target_policy)\n",
    "    print('player 2 policy: %s' % opp_target_policy)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58d82306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 expected value: -0.0668603768627914\n",
      "player 2 expected value: 0.0668603768627914\n",
      "\n",
      "player 1 strategies:\n",
      "0      ['0.50', '0.50']\n",
      "0 pb   ['1.00', '0.00']\n",
      "1      ['0.62', '0.38']\n",
      "1 pb   ['0.92', '0.08']\n",
      "2      ['0.85', '0.15']\n",
      "2 pb   ['0.69', '0.31']\n",
      "3      ['0.95', '0.05']\n",
      "3 pb   ['0.54', '0.46']\n",
      "4      ['0.92', '0.08']\n",
      "4 pb   ['0.39', '0.61']\n",
      "5      ['0.57', '0.43']\n",
      "5 pb   ['0.14', '0.86']\n",
      "6      ['0.25', '0.75']\n",
      "6 pb   ['0.04', '0.96']\n",
      "7      ['0.36', '0.64']\n",
      "7 pb   ['0.02', '0.98']\n",
      "8      ['0.33', '0.67']\n",
      "8 pb   ['0.01', '0.99']\n",
      "9      ['0.37', '0.63']\n",
      "9 pb   ['0.00', '1.00']\n",
      "\n",
      "player 2 strategies:\n",
      "0 b    ['1.00', '0.00']\n",
      "0 p    ['0.08', '0.92']\n",
      "1 b    ['0.95', '0.05']\n",
      "1 p    ['0.55', '0.45']\n",
      "2 b    ['0.78', '0.22']\n",
      "2 p    ['0.84', '0.16']\n",
      "3 b    ['0.67', '0.33']\n",
      "3 p    ['0.95', '0.05']\n",
      "4 b    ['0.23', '0.77']\n",
      "4 p    ['0.93', '0.07']\n",
      "5 b    ['0.13', '0.87']\n",
      "5 p    ['0.39', '0.61']\n",
      "6 b    ['0.03', '0.97']\n",
      "6 p    ['0.07', '0.93']\n",
      "7 b    ['0.02', '0.98']\n",
      "7 p    ['0.04', '0.96']\n",
      "8 b    ['0.01', '0.99']\n",
      "8 p    ['0.02', '0.98']\n",
      "9 b    ['0.00', '1.00']\n",
      "9 p    ['0.00', '1.00']\n",
      "7.642899990081787\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "class PokerB:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nodeMap = {}\n",
    "        self.expected_game_value = 0\n",
    "        self.n_cards = 10\n",
    "        self.nash_equilibrium = dict()\n",
    "        self.current_player = 0\n",
    "        self.deck = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "        self.n_actions = 2\n",
    "    \n",
    "    def randomHand(self):\n",
    "        for i in range(1,2):\n",
    "            r = np.random(2598960)\n",
    "            if r<1302540:\n",
    "                  self.deck[i]=0\n",
    "            if 1302540<r<1302540+1098240:\n",
    "                self.deck[i]=1\n",
    "            if 1302540+1098240<r<1302540+1098240+123552:\n",
    "                self.deck[i]=2\n",
    "            if 1302540+1098240+123552+54912<r<1302540+1098240+123552+54912+10200:\n",
    "                self.deck[i]=3\n",
    "            if 1302540+1098240+123552+54912+10200<r<1302540+1098240+123552+54912+10200+5108:\n",
    "                self.deck[i]=4\n",
    "            if 1302540+1098240+123552+54912+10200+5108<r<1302540+1098240+123552+54912+10200+5108+3744:\n",
    "                self.deck[i]=5\n",
    "            if 1302540+1098240+123552+54912+10200+5108+3744<r<1302540+1098240+123552+54912+10200+5108+3744+624:\n",
    "                self.deck[i]=6\n",
    "            if 1302540+1098240+123552+54912+10200+5108+3744+624<r<1302540+1098240+123552+54912+10200+5108+3744+624+36:\n",
    "                self.deck[i]=7\n",
    "            if 1302540+1098240+123552+54912+10200+5108+3744+624+36<r<1302540+1098240+123552+54912+10200+5108+3744+624+36+4:\n",
    "                self.deck[i]=8\n",
    "            if r>1302540+1098240+123552+54912+10200+5108+3744+624+36+4:\n",
    "                self.deck[i]=9\n",
    "        \n",
    "        \n",
    "    def train(self, n_iterations=50000):\n",
    "        expected_game_value = 0\n",
    "        for _ in range(n_iterations):\n",
    "            shuffle(self.deck)\n",
    "            expected_game_value += self.cfr('', 1, 1)\n",
    "            for _, v in self.nodeMap.items():\n",
    "                v.update_strategy()\n",
    "\n",
    "        expected_game_value /= n_iterations\n",
    "        display_results(expected_game_value, self.nodeMap)\n",
    "\n",
    "    def cfr(self, history, pr_1, pr_2):\n",
    "        n = len(history)\n",
    "        is_player_1 = n % 2 == 0\n",
    "        player_card = self.deck[0] if is_player_1 else self.deck[1]  \n",
    "\n",
    "        if self.is_terminal(history):\n",
    "            card_player = self.deck[0] if is_player_1 else self.deck[1]\n",
    "            card_opponent = self.deck[1] if is_player_1 else self.deck[0]\n",
    "            reward = self.get_reward(history, card_player, card_opponent)\n",
    "            return reward\n",
    "\n",
    "        node = self.get_node(player_card, history)\n",
    "        strategy = node.strategy\n",
    "\n",
    "        # Counterfactual utility per action.\n",
    "        action_utils = np.zeros(self.n_actions)\n",
    "\n",
    "        for act in range(self.n_actions):\n",
    "            next_history = history + node.action_dict[act]\n",
    "            if is_player_1:\n",
    "                action_utils[act] = -1 * self.cfr(next_history, pr_1 * strategy[act], pr_2)\n",
    "            else:\n",
    "                action_utils[act] = -1 * self.cfr(next_history, pr_1, pr_2 * strategy[act])\n",
    "\n",
    "        # Utility of information set.\n",
    "        util = sum(action_utils * strategy)\n",
    "        regrets = action_utils - util\n",
    "        if is_player_1:\n",
    "            node.reach_pr += pr_1\n",
    "            node.regret_sum += pr_2 * regrets\n",
    "        else:\n",
    "            node.reach_pr += pr_2\n",
    "            node.regret_sum += pr_1 * regrets\n",
    "\n",
    "        return util\n",
    "\n",
    "    @staticmethod\n",
    "    def is_terminal(history):\n",
    "        if history[-2:] == 'pp' or history[-2:] == \"bb\" or history[-2:] == 'bp':\n",
    "            return True\n",
    "\n",
    "    @staticmethod\n",
    "    def get_reward(history, player_card, opponent_card):\n",
    "        terminal_pass = history[-1] == 'p'\n",
    "        double_bet = history[-2:] == \"bb\"\n",
    "        if terminal_pass:\n",
    "            if history[-2:] == 'pp':\n",
    "                if player_card > opponent_card:\n",
    "                    return 1\n",
    "                if player_card==opponent_card:\n",
    "                    return (0)\n",
    "                else :\n",
    "                    return (-1)\n",
    "            else:\n",
    "                return 1\n",
    "        elif double_bet:\n",
    "            if player_card > opponent_card:\n",
    "                return 2\n",
    "            if player_card==opponent_card:\n",
    "                return (0)\n",
    "            else :\n",
    "                return (-2)\n",
    "\n",
    "\n",
    "    def get_node(self, card, history):\n",
    "        key = str(card) + \" \" + history\n",
    "        if key not in self.nodeMap:\n",
    "            action_dict = {0: 'p', 1: 'b'}\n",
    "            info_set = Node(key, action_dict)\n",
    "            self.nodeMap[key] = info_set\n",
    "            return info_set\n",
    "        return self.nodeMap[key]\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, key, action_dict, n_actions=2):\n",
    "        self.key = key\n",
    "        self.n_actions = n_actions\n",
    "        self.regret_sum = np.zeros(self.n_actions)\n",
    "        self.strategy_sum = np.zeros(self.n_actions)\n",
    "        self.action_dict = action_dict\n",
    "        self.strategy = np.repeat(1/self.n_actions, self.n_actions)\n",
    "        self.reach_pr = 0\n",
    "        self.reach_pr_sum = 0\n",
    "\n",
    "    def update_strategy(self):\n",
    "        self.strategy_sum += self.reach_pr * self.strategy\n",
    "        self.reach_pr_sum += self.reach_pr\n",
    "        self.strategy = self.get_strategy()\n",
    "        self.reach_pr = 0\n",
    "\n",
    "    def get_strategy(self):\n",
    "        regrets = self.regret_sum\n",
    "        regrets[regrets < 0] = 0\n",
    "        normalizing_sum = sum(regrets)\n",
    "        if normalizing_sum > 0:\n",
    "            return regrets / normalizing_sum\n",
    "        else:\n",
    "            return np.repeat(1/self.n_actions, self.n_actions)\n",
    "\n",
    "    def get_average_strategy(self):\n",
    "        strategy = self.strategy_sum / self.reach_pr_sum\n",
    "        # Re-normalize\n",
    "        total = sum(strategy)\n",
    "        strategy /= total\n",
    "        return strategy\n",
    "\n",
    "    def __str__(self):\n",
    "        strategies = ['{:03.2f}'.format(x)\n",
    "                      for x in self.get_average_strategy()]\n",
    "        return '{} {}'.format(self.key.ljust(6), strategies)\n",
    "\n",
    "\n",
    "def display_results(ev, i_map):\n",
    "    print('player 1 expected value: {}'.format(ev))\n",
    "    print('player 2 expected value: {}'.format(-1 * ev))\n",
    "\n",
    "    print()\n",
    "    print('player 1 strategies:')\n",
    "    sorted_items = sorted(i_map.items(), key=lambda x: x[0])\n",
    "    for _, v in filter(lambda x: len(x[0]) % 2 == 0, sorted_items):\n",
    "        print(v)\n",
    "    print()\n",
    "    print('player 2 strategies:')\n",
    "    for _, v in filter(lambda x: len(x[0]) % 2 == 1, sorted_items):\n",
    "        print(v)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    time1 = time.time()\n",
    "    trainer = PokerB()\n",
    "    trainer.train(n_iterations=25000)\n",
    "    print(abs(time1 - time.time()))\n",
    "    print(sys.getsizeof(trainer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbabc1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "class Kunh:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nodeMap = {}\n",
    "        self.expected_game_value = 0\n",
    "        self.n_cards = 10\n",
    "        self.nash_equilibrium = dict()\n",
    "        self.current_player = 0\n",
    "        self.deck = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "        self.n_actions = 2\n",
    "    \n",
    "    def randomHand(self):\n",
    "        for i in range(1,2):\n",
    "            r = np.random(2598960)\n",
    "            if r<1302540:\n",
    "                  self.deck[i]=0\n",
    "            if 1302540<r<1302540+1098240:\n",
    "                self.deck[i]=1\n",
    "            if 1302540+1098240<r<1302540+1098240+123552:\n",
    "                self.deck[i]=2\n",
    "            if 1302540+1098240+123552+54912<r<1302540+1098240+123552+54912+10200:\n",
    "                self.deck[i]=3\n",
    "            if 1302540+1098240+123552+54912+10200<r<1302540+1098240+123552+54912+10200+5108:\n",
    "                self.deck[i]=4\n",
    "            if 1302540+1098240+123552+54912+10200+5108<r<1302540+1098240+123552+54912+10200+5108+3744:\n",
    "                self.deck[i]=5\n",
    "            if 1302540+1098240+123552+54912+10200+5108+3744<r<1302540+1098240+123552+54912+10200+5108+3744+624:\n",
    "                self.deck[i]=6\n",
    "            if 1302540+1098240+123552+54912+10200+5108+3744+624<r<1302540+1098240+123552+54912+10200+5108+3744+624+36:\n",
    "                self.deck[i]=7\n",
    "            if 1302540+1098240+123552+54912+10200+5108+3744+624+36<r<1302540+1098240+123552+54912+10200+5108+3744+624+36+4:\n",
    "                self.deck[i]=8\n",
    "            if r>1302540+1098240+123552+54912+10200+5108+3744+624+36+4:\n",
    "                self.deck[i]=9\n",
    "        \n",
    "        \n",
    "    def train(self, n_iterations=50000):\n",
    "        expected_game_value = 0\n",
    "        for _ in range(n_iterations):\n",
    "            shuffle(self.deck)\n",
    "            expected_game_value += self.cfr('', 1, 1)\n",
    "            for _, v in self.nodeMap.items():\n",
    "                v.update_strategy()\n",
    "\n",
    "        expected_game_value /= n_iterations\n",
    "        display_results(expected_game_value, self.nodeMap)\n",
    "\n",
    "    def cfr(self, history, pr_1, pr_2):\n",
    "        n = len(history)\n",
    "        is_player_1 = n % 2 == 0\n",
    "        player_card = self.deck[0] if is_player_1 else self.deck[1]  \n",
    "\n",
    "        if self.is_terminal(history):\n",
    "            card_player = self.deck[0] if is_player_1 else self.deck[1]\n",
    "            card_opponent = self.deck[1] if is_player_1 else self.deck[0]\n",
    "            reward = self.get_reward(history, card_player, card_opponent)\n",
    "            return reward\n",
    "\n",
    "        node = self.get_node(player_card, history)\n",
    "        strategy = node.strategy\n",
    "\n",
    "        # Counterfactual utility per action.\n",
    "        action_utils = np.zeros(self.n_actions)\n",
    "\n",
    "        for act in range(self.n_actions):\n",
    "            next_history = history + node.action_dict[act]\n",
    "            if is_player_1:\n",
    "                action_utils[act] = -1 * self.cfr(next_history, pr_1 * strategy[act], pr_2)\n",
    "            else:\n",
    "                action_utils[act] = -1 * self.cfr(next_history, pr_1, pr_2 * strategy[act])\n",
    "\n",
    "        # Utility of information set.\n",
    "        util = sum(action_utils * strategy)\n",
    "        regrets = action_utils - util\n",
    "        if is_player_1:\n",
    "            node.reach_pr += pr_1\n",
    "            node.regret_sum += pr_2 * regrets\n",
    "        else:\n",
    "            node.reach_pr += pr_2\n",
    "            node.regret_sum += pr_1 * regrets\n",
    "\n",
    "        return util\n",
    "\n",
    "    @staticmethod\n",
    "    def is_terminal(history):\n",
    "        if history[-2:] == 'pp' or history[-2:] == \"bb\" or history[-2:] == 'bp':\n",
    "            return True\n",
    "    \n",
    "    def suitR(a):\n",
    "        if a==s:\n",
    "            return(4)\n",
    "        if a==h:\n",
    "            return(3)\n",
    "        if a==d:\n",
    "            return(2)\n",
    "        if a==c:\n",
    "            return(1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def Handchecker(a,b):\n",
    "        a=sorted(a,key=lambda l:l[1], reverse=True)\n",
    "        k=0\n",
    "        s=0\n",
    "        for i in range(0,4):\n",
    "            if a[i,0]==14-i:\n",
    "                k=k+1\n",
    "        if k==5:\n",
    "            for i in range(0,3):\n",
    "                if a[i,1]==a[i+1,1]:\n",
    "                    s=s+1  \n",
    "        if s==3:\n",
    "            return([10,suitR(a[0,1]),0,0,0,0])\n",
    "        \n",
    "        k=0\n",
    "        s=0\n",
    "        for i in range(0,3):\n",
    "            if a[i,0]==a[i+1]-1:\n",
    "                k=k+1\n",
    "        if k==5:\n",
    "            for i in range(0,3):\n",
    "                if a[i,1]==a[i+1,1]:\n",
    "                    s=s+1 \n",
    "        if s==3:\n",
    "            return([9,suitR(a[0,1]),0,0,0,0])\n",
    "        \n",
    "        k=0\n",
    "        s=0\n",
    "        if i in range(0,4):\n",
    "            if a[i,0]==a[i+1,0]:\n",
    "                k=k+1\n",
    "            else:\n",
    "                s=i\n",
    "        if k==3:\n",
    "            return([8,0,0,0,a[s,0],suitR(a[0,1])])\n",
    "        \n",
    "        k=0\n",
    "        s=0\n",
    "        if i in range(0,4):\n",
    "            for j in range(0,4):\n",
    "                k=0\n",
    "                if a[i,0]==a[j,0]:\n",
    "                    k=k+1\n",
    "                if k==3:\n",
    "                    s=1\n",
    "        if s=1:\n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_reward(history, player_card, opponent_card):\n",
    "        terminal_pass = history[-1] == 'p'\n",
    "        double_bet = history[-2:] == \"bb\"\n",
    "        if terminal_pass:\n",
    "            if history[-2:] == 'pp':\n",
    "                if player_card > opponent_card:\n",
    "                    return 1\n",
    "                if player_card==opponent_card:\n",
    "                    return (0)\n",
    "                else :\n",
    "                    return (-1)\n",
    "            else:\n",
    "                return 1\n",
    "        elif double_bet:\n",
    "            if player_card > opponent_card:\n",
    "                return 2\n",
    "            if player_card==opponent_card:\n",
    "                return (0)\n",
    "            else :\n",
    "                return (-2)\n",
    "\n",
    "\n",
    "    def get_node(self, card, history):\n",
    "        key = str(card) + \" \" + history\n",
    "        if key not in self.nodeMap:\n",
    "            action_dict = {0: 'p', 1: 'b'}\n",
    "            info_set = Node(key, action_dict)\n",
    "            self.nodeMap[key] = info_set\n",
    "            return info_set\n",
    "        return self.nodeMap[key]\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, key, action_dict, n_actions=2):\n",
    "        self.key = key\n",
    "        self.n_actions = n_actions\n",
    "        self.regret_sum = np.zeros(self.n_actions)\n",
    "        self.strategy_sum = np.zeros(self.n_actions)\n",
    "        self.action_dict = action_dict\n",
    "        self.strategy = np.repeat(1/self.n_actions, self.n_actions)\n",
    "        self.reach_pr = 0\n",
    "        self.reach_pr_sum = 0\n",
    "\n",
    "    def update_strategy(self):\n",
    "        self.strategy_sum += self.reach_pr * self.strategy\n",
    "        self.reach_pr_sum += self.reach_pr\n",
    "        self.strategy = self.get_strategy()\n",
    "        self.reach_pr = 0\n",
    "\n",
    "    def get_strategy(self):\n",
    "        regrets = self.regret_sum\n",
    "        regrets[regrets < 0] = 0\n",
    "        normalizing_sum = sum(regrets)\n",
    "        if normalizing_sum > 0:\n",
    "            return regrets / normalizing_sum\n",
    "        else:\n",
    "            return np.repeat(1/self.n_actions, self.n_actions)\n",
    "\n",
    "    def get_average_strategy(self):\n",
    "        strategy = self.strategy_sum / self.reach_pr_sum\n",
    "        # Re-normalize\n",
    "        total = sum(strategy)\n",
    "        strategy /= total\n",
    "        return strategy\n",
    "\n",
    "    def __str__(self):\n",
    "        strategies = ['{:03.2f}'.format(x)\n",
    "                      for x in self.get_average_strategy()]\n",
    "        return '{} {}'.format(self.key.ljust(6), strategies)\n",
    "\n",
    "\n",
    "def display_results(ev, i_map):\n",
    "    print('player 1 expected value: {}'.format(ev))\n",
    "    print('player 2 expected value: {}'.format(-1 * ev))\n",
    "\n",
    "    print()\n",
    "    print('player 1 strategies:')\n",
    "    sorted_items = sorted(i_map.items(), key=lambda x: x[0])\n",
    "    for _, v in filter(lambda x: len(x[0]) % 2 == 0, sorted_items):\n",
    "        print(v)\n",
    "    print()\n",
    "    print('player 2 strategies:')\n",
    "    for _, v in filter(lambda x: len(x[0]) % 2 == 1, sorted_items):\n",
    "        print(v)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    time1 = time.time()\n",
    "    trainer = Kunh()\n",
    "    trainer.train(n_iterations=25000)\n",
    "    print(abs(time1 - time.time()))\n",
    "    print(sys.getsizeof(trainer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
